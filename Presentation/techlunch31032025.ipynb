{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Automating online data collection using web-scraping\"\n",
        "subtitle: \"Tech Lunch @ CBDR\"\n",
        "author: \"Sabou\"\n",
        "date: today\n",
        "format: \n",
        "  revealjs:\n",
        "    theme: [default,style.scss]\n",
        "    slide-number: true\n",
        "    scrollable: true\n",
        "controls: true\n",
        "editor: source\n",
        "date-format: \"DD.MM.YY\"\n",
        "---"
      ],
      "id": "a9235904"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Why?\n",
        "\n",
        ":::{.fragment .fade-up}\n",
        "![](images/fry.png){height=600}\n",
        ":::\n",
        "\n",
        "## Our program\n",
        "\n",
        "1.  Why would I want to automate web-scraping?\n",
        "2.  The right tool for the jobâ€¦\n",
        "3.  â€¦ but how do I know the job?\n",
        "4.  Some practical examples and code\n",
        "5.  Q&A\n",
        "\n",
        "## What I used it for {.incremental}\n",
        "\n",
        "::: {.incremental}\n",
        "* exporting stock developments\n",
        "* determining median hotel prices\n",
        "* extracting word frequency\n",
        "* circumventing copyright protection (flipbook) ðŸ¤«\n",
        "* making a ðŸ’©ton of data entries\n",
        ":::\n",
        "\n",
        "<!--\n",
        "Exporting stock developments for **predictive analytics** in Renatoâ€™s forecasting seminar.\n",
        "\n",
        "\n",
        "\n",
        "## Some inspiration for what you could do...\n",
        "\n",
        "\n",
        "\n",
        "Determining the **median hotel price** at a specific location during a specific time period (hello Master's thesis ðŸ˜‰)\n",
        "\n",
        "## Some inspiration for what you could do...\n",
        "\n",
        "Extracting frequency statistics on word use from a webpage without an export button\n",
        "\n",
        "\n",
        "## Some inspiration for what you could do...\n",
        "<br>\n",
        "<br>\n",
        "Extracting stupidly designed .svg and .jpg images from an online educational resource because you refuse to accept that they no longer sell e-books as PDFs but instead provide them on a website as a subscription service\n",
        "\n",
        "\n",
        "## Some inspiration for what you could do...\n",
        "<br>\n",
        "<br>\n",
        "Faking 25â€™000 data entries in an online tool that doesnâ€™t allow data upload / import to simulate a benchmark study\n",
        "-->\n",
        "\n",
        "\n",
        "## In general\n",
        "\n",
        "-   Structured data from webpages without export button\n",
        "-   Analyzing online-data (forums, product reviews, â€¦)\n",
        "-   Systematic interactions with specific sites\n",
        "-   Fun personal projects ðŸ˜Š\n",
        "-   ðŸ’­...\n",
        "\n",
        "**But!! Be mindful of the ethical, regulatory and legal impact of your project.**\n",
        "\n",
        "## The right tool for the job...\n",
        "\n",
        "![](images/choicetool.svg)\n",
        "\n",
        "## Selenium\n",
        "\n",
        "> automated interaction with a web-browser\n",
        "\n",
        ":::{.columns}\n",
        "\n",
        ":::{.column width=60%}\n",
        "\n",
        "### Pro\n",
        "* interaction with web-browser\n",
        "* input data (e.g. forms, text)\n",
        "* interaction with dynamic sites (JavaScript)\n",
        "\n",
        ":::\n",
        ":::{.column width=40%}\n",
        "\n",
        "### Con\n",
        "* slow \n",
        "* learning curve\n",
        "* pain in the butt sometimes\n",
        "\n",
        ":::\n",
        ":::\n",
        "\n",
        "## BeautifulSoup4 \n",
        "\n",
        "> extracts HTML structures and contents from webpages\n",
        "\n",
        ":::{.columns}\n",
        "\n",
        ":::{.column width=50%}\n",
        "\n",
        "### Pro\n",
        "* extracting html components of a single URL / a list of URLs where structure is standardized\n",
        "* easy translations of objects into dataframes\n",
        "\n",
        ":::\n",
        ":::{.column width=50%}\n",
        "\n",
        "### Con\n",
        "* interaction with web-browser\n",
        "* input data (e.g. forms, text)\n",
        "* interaction with dynamic sites (JavaScript)\n",
        "\n",
        ":::\n",
        ":::\n",
        "\n",
        "<!--\n",
        "## ... but how do I know the job?\n",
        "\n",
        "+-------------------------------------------------------------------------------------------------+-------------------------------------------------+\n",
        "| BeautifulSoup4 (Rvest)                                                                          | Selenium                                        |\n",
        "+=================================================================================================+=================================================+\n",
        "| extracts HTML structures and contents from webpages                                             | automated interaction with a web-browser        |\n",
        "+-------------------------------------------------------------------------------------------------+-------------------------------------------------+\n",
        "| -   extracting html components of a single URL / a list of URLs where structure is standardized | -   interaction with web-browser                |\n",
        "|                                                                                                 |                                                 |\n",
        "| -   easy translations of objects into dataframes                                                | -   input data (e.g. forms, text)               |\n",
        "|                                                                                                 |                                                 |\n",
        "|                                                                                                 | -   interaction with dynamic sites (JavaScript) |\n",
        "+-------------------------------------------------------------------------------------------------+-------------------------------------------------+\n",
        "| -   only useful if structured data                                                              | -   slow                                        |\n",
        "|                                                                                                 |                                                 |\n",
        "| -   only useful for static websites                                                             | -   learning curve                              |\n",
        "|                                                                                                 |                                                 |\n",
        "| -   pain in the butt sometimes                                                                  | -   pain in the butt sometimes                  |\n",
        "+-------------------------------------------------------------------------------------------------+-------------------------------------------------+\n",
        "-->\n",
        "\n",
        "## Beautifulsoup\n",
        "\n",
        "How many chairs (and \"sub-chairs\") do we have at the psychological Institute in Zurich?\n",
        "\n",
        ". . .\n",
        "\n",
        "\n",
        "a. 5 chairs and 35 \"sub-chairs\"\n",
        "b. 5 chairs and 20 \"sub-chairs\"\n",
        "c. 5 chairs and 29 \"sub-chairs\"\n",
        "\n",
        "## Beautifulsoup\n"
      ],
      "id": "88b05ae8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "base_url = \"https://www.psychologie.uzh.ch\"\n",
        "overview_url = base_url + \"/de/bereiche/uebersicht.html\"\n",
        "\n",
        "response = requests.get(overview_url)\n",
        "soup = BeautifulSoup(response.text,\"html.parser\")\n",
        "\n",
        "\n",
        "\n",
        "## Level 1 subpages\n",
        "subpage_links = []\n",
        "all_data = []\n",
        "\n",
        "\n",
        "for link in soup.find_all(\"a\",href=True):\n",
        "    href = link[\"href\"]\n",
        "    if href.startswith(\"/de/bereiche/\") and href.endswith(\".html\"):\n",
        "        subpage_links.append(base_url+href)\n",
        "        print(base_url+href)\n",
        "\n",
        "for subpage_url in subpage_links:\n",
        "    \n",
        "    print(f\"\\nScraping {subpage_url}\")\n",
        "    response = requests.get(subpage_url)\n",
        "    sub_soup = BeautifulSoup(response.text,\"html.parser\")\n",
        "\n",
        "    chair = sub_soup.find(\"h1\") ## e.g. Entwicklungspsychologie\n",
        "    chair_name = chair.get_text(strip=True)\n",
        "    print(chair_name)\n",
        "\n",
        "    tables = sub_soup.find_all(\"table\", class_=\"basic\") ## e.g. \"Entwicklungspsychologie: Kinder- und SÃ¤uglingsalter\"\n",
        "\n",
        "    for i, table in enumerate(tables):\n",
        "        print(f\"\\nTable {i+1} in {subpage_url}\")\n",
        "        table_data = []\n",
        "        table_data.append(chair_name)\n",
        "        rows = table.find_all(\"tr\")\n",
        "\n",
        "        for i, row in enumerate(rows):\n",
        "        \n",
        "            cells = row.find_all([\"th\",\"td\"])\n",
        "            length = len(cells)\n",
        "            print(f\"\\nlen: {length}\")\n",
        "\n",
        "            if length == 2:\n",
        "                title = cells[0].get_text(strip=True) if len(cells) > 0 else \"\"\n",
        "                table_data.append(title)\n",
        "                link = cells[1].find(\"a\",href=True)\n",
        "                table_data.append(link)\n",
        "                print(f\"\\niteration: {i}\")\n",
        "                print(\"Length = 2\")\n",
        "                print(f\"\\nTitle: {title}\")\n",
        "                print(f\"\\nLink: {link}\")\n",
        "                if link:\n",
        "                    full_description = []\n",
        "                    subsubpage_link = base_url + link[\"href\"]\n",
        "                    subresponse = requests.get(subsubpage_link)\n",
        "                    subsub_soup = BeautifulSoup(subresponse.text, \"html.parser\")\n",
        "                    description = subsub_soup.find(\"section\", class_=\"ContentArea\")\n",
        "                    \n",
        "                    if description:\n",
        "                        section_text = description.get_text(strip=True, separator=\" \")\n",
        "                        full_description.append(section_text)\n",
        "                        \n",
        "                    else:\n",
        "                        print(\"No section found\")\n",
        "                    description_text = \" \".join(full_description)\n",
        "                    table_data.append(description_text)\n",
        "                    print(f\"\\ndescription: {description_text}\")\n",
        "            elif length == 1 and i == 1:\n",
        "                print(f\"\\niteration: {i}\")\n",
        "                print(\"Length = 1\")\n",
        "                name = cells[0].get_text(strip=True) if len(cells) > 0 else \"\"\n",
        "                print(f\"\\nName: {name}\")\n",
        "                table_data.append(name)\n",
        "            elif length == 1 and i == 2:\n",
        "                print(f\"\\niteration: {i}\")\n",
        "                print(\"Length = 1\")\n",
        "                position = cells[0].get_text(strip=True) if len(cells) > 0 else \"\"\n",
        "                print(f\"\\nPosition: {position}\")\n",
        "                table_data.append(position)\n",
        "            \n",
        "            print(table_data)\n",
        "            all_data.append(table_data)\n",
        "\n",
        "df = pd.DataFrame(all_data)\n",
        "df.to_csv(\"beautifulsoup.csv\",index=False,sep=\";\",quoting=1,encoding=\"utf-8-sig\")"
      ],
      "id": "3bc578b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Selenium\n",
        "\n",
        "What is the most-watched video on the youtube channel of UZH?\n",
        "\n",
        "a. Der flexible Schweif des Prions vergiftet die Hirnzellen (*en: the flexible tail of the prion poisons brain cells*)\n",
        "b. Schwebebahn mit Hochtemperatur-Supraleitung (*en: Suspension railroad with high-temperature superconductivity*)\n",
        "c. Moosforschung (*en: moss research*)\n",
        "\n",
        "## Selenium\n",
        "\n",
        "``` python\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.firefox.service import Service\n",
        "from webdriver_manager.firefox import GeckoDriverManager\n",
        "from selenium.webdriver.firefox.options import Options\n",
        "import time\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "data = [] \n",
        "def text_before(text,delimiter):\n",
        "    return text.split(delimiter)[0] if delimiter in text else text\n",
        "def text_after(text, delimiter):\n",
        "    return text.split(delimiter, 1)[1] if delimiter in text else \"\"\n",
        "\n",
        "firefox_options = Options()\n",
        "#firefox_options.add_argument(\"--headless\")  # if you don't want to open a window\n",
        "firefox_options.add_argument(\"--disable-gpu\")\n",
        "\n",
        "driver = webdriver.Firefox(service=Service(GeckoDriverManager().install()), options=firefox_options)\n",
        "\n",
        "channel_url = 'https://www.youtube.com/@uzhch/videos'\n",
        "driver.get(channel_url)\n",
        "time.sleep(5)\n",
        "\n",
        "## accept all cookies\n",
        "try:\n",
        "    accept_button = driver.find_element(By.XPATH, \"/html/body/c-wiz/div/div/div/div[2]/div[1]/div[3]/div[1]/form[2]/div/div/button/span\")\n",
        "    accept_button.click()\n",
        "    print(\"Cookies accepted.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not accept cookies: {e}\")\n",
        "\n",
        "time.sleep(5)\n",
        "\n",
        "start_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "print(start_height)\n",
        "max_attempts = 100\n",
        "attempt = 0\n",
        "\n",
        "while attempt < max_attempts:    \n",
        "    html = driver.find_element(By.TAG_NAME, 'html')\n",
        "    html.send_keys(Keys.END)\n",
        "    time.sleep(3)\n",
        "    new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "    print(new_height)\n",
        "    if new_height == start_height:\n",
        "        break\n",
        "    else: \n",
        "        start_height = new_height\n",
        "\n",
        "    attempt+=1\n",
        "\n",
        "links = driver.find_elements(By.CSS_SELECTOR, \"a#video-title-link\")\n",
        "aria_labels = [link.get_attribute(\"aria-label\") for link in links]\n",
        "for label in aria_labels:\n",
        "    print(label)\n",
        "\n",
        "    title = text_before(label,\"by UniversitÃ¤t ZÃ¼rich\")\n",
        "    print(title)\n",
        "    \n",
        "    views = text_after(label,\"by UniversitÃ¤t ZÃ¼rich\")\n",
        "    views = text_before(views,\" views \")\n",
        "    print(views)\n",
        "    \n",
        "    date = text_after(label, views+\" views \")\n",
        "    date = text_before(date,\" ago \")\n",
        "    print(date)\n",
        "    \n",
        "    duration = text_after(label,date+\" ago \")\n",
        "    print(duration)\n",
        "\n",
        "    data.append([title, views, date, duration])\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"Title\", \"Views\", \"Date\", \"Duration\"])\n",
        "df.to_csv(\"youtube_data.csv\",index=False,quoting=1,sep=\";\",encoding=\"utf-8\")\n",
        "```\n",
        "\n",
        "\n",
        "## If you need more\n",
        "\n",
        "$\\Longrightarrow$ Scrapy to crawl **entire** websites and **extract everything** (useful for large data gathering for ML etc.)\n",
        "\n",
        ":::mittig\n",
        "![](https://rlv.zcache.com/pirate_penguin_photo_sculpture_cutouts-r47aec11efd3c4192a0621d611fc5082e_x7saw_8byvr_644.webp)\n",
        ":::\n",
        "\n",
        "## Q&A"
      ],
      "id": "ca145bd9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}